{"cells":[{"metadata":{"_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a","_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"},"cell_type":"markdown","source":"# Toxic Comment Challenge - A Multilabel Classification Problem\n#### Authored by Megan Yow\nJun 18, 2020  \nV1 - initial run of notebook  \nV2 - Made Submission Files for Binary and Chain Classification\nV3 - Advanced Setting for saving output (no code changes made)\n\nThis kernel is inspired by:\n- kernel by Jeremy Howard : _NB-SVM strong linear baseline + EDA (0.052 lb)_\n- kernel by Issac : _logistic regression (0.055 lb)_\n- _Solving Multi-Label Classification problems_, https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n- **Heavily Inspired** notebook by Rhodium Beng: Classifying multi-label comments (0.9741 lb) \n- submitting from a kernel  https://www.kaggle.com/dansbecker/submitting-from-a-kernel + Advanced Settings (save output for this version)","execution_count":null},{"metadata":{"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"},"cell_type":"markdown","source":"## Load training and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.chdir('../input/jigsaw-toxic-comment-classification-challenge/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","_cell_guid":"094fff47-db10-447c-965e-08056f718bde","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"{:03.2f} MB\".format(train_df.memory_usage(deep=True).sum() / 1024) # usage in bytes, MB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"},"cell_type":"markdown","source":"## Examine the data (EDA)","execution_count":null},{"metadata":{"_uuid":"9e53b7599d707a9420a75c37c7ac6d05bed9df7b","_kg_hide-output":true,"_cell_guid":"c4c7137d-6bc7-4b41-b50a-e511883155e9","trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a","_cell_guid":"40597119-1274-4d5b-a054-b4b17dbcbb36"},"cell_type":"markdown","source":"In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem.","execution_count":null},{"metadata":{"_uuid":"7f7f2581edb2f42a64812dec31622a011dceff80","_cell_guid":"7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e","trusted":true},"cell_type":"code","source":"cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a226272e319458391e117e8fb7f16b17c4884f","_cell_guid":"ce00e980-da07-4412-ae4c-5152cc2036e0","trusted":true},"cell_type":"code","source":"# check missing values in numeric columns\ntrain_df.isna().sum() # no missing data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe() #tag rates of each column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0a52911f77fade7ece075fdc0d9df9433029eec"},"cell_type":"code","source":"unlabelled_in_all = train_df[(train_df['toxic']!=1) & (train_df['severe_toxic']!=1) & (train_df['obscene']!=1) & \n                            (train_df['threat']!=1) & (train_df['insult']!=1) & (train_df['identity_hate']!=1)]\nprint('Percentage of unlabelled comments is ', len(unlabelled_in_all)/len(train_df)*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c616bcb0dcf611679dcb5009def9d71d6727cd0e","_cell_guid":"6bd65c22-9c8a-4756-a7bf-16187a6044d1","trusted":true},"cell_type":"code","source":"test_df.isna().sum() # no missing data in test set as well","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f","_cell_guid":"ecce8833-b5b4-40b4-a164-015b7380b8b1"},"cell_type":"markdown","source":"All rows in the training and test data contain comments, so there's no need to clean up null fields.","execution_count":null},{"metadata":{"_uuid":"e497fc6688e16602e3f49a11939f32324d295a8d","_cell_guid":"b3dcbd9f-dbb8-4a5a-96b2-b93882516e27","trusted":true},"cell_type":"code","source":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in test is {}'.format(len(test_df)))\nprint('Total rows in train is {}'.format(len(train_df)))\nprint(train_df[cols_target].sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","collapsed":true,"_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654"},"cell_type":"markdown","source":"As mentioned earlier, majority of the comments in the training data are not labelled in one or more of these categories.","execution_count":null},{"metadata":{"_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa","trusted":true},"cell_type":"code","source":"# Let's look at the character length for the rows in the training data and record these\ntrain_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","scrolled":true,"_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59","trusted":true},"cell_type":"code","source":"# look at the histogram plot for text length\nsns.set()\ntrain_df['char_length'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944","_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"},"cell_type":"markdown","source":"Most of the text length are within 500 characters, with some up to 5,000 characters long.","execution_count":null},{"metadata":{"_uuid":"e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e","_cell_guid":"f16a287f-27d4-4afa-82a1-dd441c2fd36c"},"cell_type":"markdown","source":"Next, let's examine the correlations among the target variables.","execution_count":null},{"metadata":{"_uuid":"fab22b3f850c80a10d665ae43ee09b5107a79887","_cell_guid":"64164a2c-770f-469d-9020-e91714a9b2a8","trusted":true},"cell_type":"code","source":"data = train_df[cols_target]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58968a44d8fdb3b93ac57f1ca20f81ffb71d164f","scrolled":true,"_kg_hide-output":false,"_cell_guid":"7fc7803b-a7f1-414d-84fa-d1d2201c8bb7","trusted":true},"cell_type":"code","source":"colormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9da9651b4e007088b19a06165d0727fb4f4e07","_cell_guid":"095fd031-32a0-400b-9357-af0c8f50dd6b"},"cell_type":"markdown","source":"Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult.","execution_count":null},{"metadata":{"_uuid":"b677d6a32b7b72e08ba3b46bce572a223db964de","_cell_guid":"caa00f5d-7e26-48cb-92b3-acc1f1de0aca"},"cell_type":"markdown","source":"What about the character length & distribution of the comment text in the test data?","execution_count":null},{"metadata":{"_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3","trusted":true},"cell_type":"code","source":"test_df['char_length'] = test_df['comment_text'].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.hist(test_df['char_length'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06","_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c"},"cell_type":"markdown","source":"Now, the shape of character length distribution looks similar between the training data and the train data. For the training data, I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of the comment categories.","execution_count":null},{"metadata":{"_uuid":"d88d9cea99dbd77e81a5b3c4b9309df88b04550b","_cell_guid":"fdf9d2f6-d248-452f-8f94-562755e3a3f3"},"cell_type":"markdown","source":"## Clean up the comment text","execution_count":null},{"metadata":{"_uuid":"b42586552d4cdc79793b0de8e630f863f2b2c456","_cell_guid":"24392feb-0adc-4e27-bd20-41ed8cadce37","trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[15:20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3944cb9d1311fd3729a7dfb13e8ac8c2309962e5","_cell_guid":"7b426494-c856-4cf4-b0d1-4162bc3aef5b","trusted":true},"cell_type":"code","source":"cleaned_df = train_df.copy()\ncleaned_df['comment_text'] = cleaned_df['comment_text'].map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_df[15:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672","_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110"},"cell_type":"markdown","source":"\n## Define X from entire train & test data for use in tokenization by Vectorizer","execution_count":null},{"metadata":{"_uuid":"b15dba3906f67e764af0a627e46d7b7e486888c5","_cell_guid":"30bf94de-36aa-4e8d-8d12-64172d8dc446","trusted":true},"cell_type":"code","source":"cleaned_df = cleaned_df.drop('char_length',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","trusted":true},"cell_type":"code","source":"X = cleaned_df.comment_text\ny_all = cleaned_df[cols_target]\ntest_X = test_df.comment_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1e91df6163a437c5f55e9c4de88dc11a89e5ba","_cell_guid":"083686b8-483a-4fbd-8584-cb9da8357c57","trusted":true},"cell_type":"code","source":"print(X.shape, test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"},"cell_type":"markdown","source":"## Vectorize the data","execution_count":null},{"metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb","trusted":true},"cell_type":"code","source":"# import and instantiate TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer # feature extractor\nvect = TfidfVectorizer(max_features=5000,stop_words='english') # features selected by top frequency\nvect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c","trusted":true},"cell_type":"code","source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_dtm = vect.fit_transform(X)\n# examine the document-term matrix created from X_train\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect.get_feature_names()[400:405] # first hundreds are numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4","trusted":true},"cell_type":"code","source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_X_dtm = vect.transform(test_X)\n# examine the document-term matrix from X_test\ntest_X_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98a540b42db71f1639d47f31d2a4ea851aa1b9e5","_cell_guid":"c84eb9f4-4fa5-418c-883c-9d9316092db0"},"cell_type":"markdown","source":"## Solving a multi-label classification problem\nOne way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem. {Thanks Joshua for pointing out.}","execution_count":null},{"metadata":{"_uuid":"389245398dce9573dfa0f7c2facd2849d2174f1f","_cell_guid":"7abd771a-b0f3-47bc-b4ff-13a78aff48e7"},"cell_type":"markdown","source":"## Binary Relevance - build a multi-label classifier using Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nfrom sklearn.metrics import roc_auc_score # version 0.19.1\n\ndef evaluate(y_true, y_probs):\n    macro_auc = roc_auc_score(y_true, y_probs, average='macro')\n    micro_auc = roc_auc_score(y_true, y_probs, average='micro')\n    return {'macro_auc': macro_auc, 'micro_auc': micro_auc}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe","trusted":true},"cell_type":"code","source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression() # C is inverse regularization strength based on SVM\n\n# create submission file\nsubmission_binary = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n\nfor label in cols_target:\n    print('... Processing Tag: {}'.format(label))\n    y = cleaned_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm, y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n    # compute auc\n    y_prob_X = logreg.predict_proba(X_dtm)[:,1]\n    cleaned_df[label+'_prob'] = y_prob_X\n    print('Training AUC is {}'.format(evaluate(np.array(y),y_prob_X)))\n    # compute the predicted probabilities for X_test_dtm\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_binary[label] = test_y_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_probs = ['obscene_prob','insult_prob','toxic_prob','severe_toxic_prob','identity_hate_prob','threat_prob']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(np.array(y_all), cleaned_df[cols_probs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_binary.to_csv('submission_binary.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18644d484121f3bab7a06207bd7fa739b15feff5","_cell_guid":"be8c8e90-f798-46d8-b77c-cd0668292646"},"cell_type":"markdown","source":"#### Binary Relevance with Logistic Regression classifier scored 0.074 on the public leaderboard.","execution_count":null},{"metadata":{"_uuid":"0393cd387f508609c0d068c68fb7dbb0be659383","_cell_guid":"5018c7ea-dd27-4d4c-b9ad-ef2140c773e5"},"cell_type":"markdown","source":"## Classifier Chains - build a multi-label classifier using Logistic Regression","execution_count":null},{"metadata":{"_uuid":"f1b4fea94c83661d7bbad5c6bc7a5994643128e2","_cell_guid":"b2172222-42b8-4f0a-8d20-160d52af6f62","trusted":true},"cell_type":"code","source":"# create submission file\nsubmission_chains = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n\n# create a function to add features\ndef add_feature(X, feature_to_add):\n    '''\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    '''\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cdc6de563643c7a86f0c54ecff2f720695fe81d","_cell_guid":"20c3ff4a-8925-4c78-a8f1-74f080f0b890","trusted":true},"cell_type":"code","source":"for label in cols_target:\n    print('... Processing {}'.format(label))\n    y = cleaned_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm,y)\n    \n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n    \n    # compute the training AUC\n    y_prob_X = logreg.predict_proba(X_dtm)[:,1]\n    cleaned_df[label+'_prob2'] = y_prob_X\n    print('Training AUC is {}'.format(evaluate(np.array(y),y_prob_X)))\n    \n    # make predictions from test_X\n    test_y = logreg.predict(test_X_dtm)\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_chains[label] = test_y_prob\n    \n    # chain current label to X_dtm\n    X_dtm = add_feature(X_dtm, y)\n    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n    # chain current label predictions to test_X_dtm\n    test_X_dtm = add_feature(test_X_dtm, test_y)\n    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_probs = ['obscene_prob2','insult_prob2','toxic_prob2','severe_toxic_prob2','identity_hate_prob2','threat_prob2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_chains.to_csv('submission_chains.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Powerset - 63 multi class problem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain, combinations\ncols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\ndef powerset(iterable):\n    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[c for c in powerset(cols_target)][10:15] # excluding all tags unlabelled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}